import io
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from ISNeuralNetwork import raw_data, model, X_test, cause_encoder, history, processed_df
from MLRF import train_rf_model
from MLSVM import train_svm_model

import sys
sys.path.append('D:\Work\Code\data-analysis\MLSVM.py')
sys.path.append('D:\Work\Code\data-analysis\MLRF.py')
sys.path.append('D:\Work\Code\data-analysis\ISNeuralNetwork')


st.set_page_config(
    page_title="Intelligent System Project",
    page_icon="ðŸ“Š" ,
)

# Load data
@st.cache_data
def load_data(file_path):
    df = pd.read_csv(file_path)
    
    # à¸à¸³à¸«à¸™à¸”à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸¥à¸‚
    numeric_cols = ["Close/Last", "Open", "High", "Low", "Volume"]

    for col in numeric_cols:
        # à¸¥à¸šà¸ªà¸±à¸à¸¥à¸±à¸à¸©à¸“à¹Œà¸žà¸´à¹€à¸¨à¸© à¹€à¸Šà¹ˆà¸™ $ à¹à¸¥à¸°à¸Šà¹ˆà¸­à¸‡à¸§à¹ˆà¸²à¸‡
        df[col] = df[col].replace({'\\$': '', ' ': ''}, regex=True)
        
        # à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ float à¹‚à¸”à¸¢à¸à¸³à¸«à¸™à¸” errors='coerce' à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¹à¸›à¸¥à¸‡à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¹€à¸›à¹‡à¸™ NaN
        df[col] = pd.to_numeric(df[col], errors='coerce')

    # à¸¥à¸šà¹à¸–à¸§à¸—à¸µà¹ˆà¸¡à¸µà¸„à¹ˆà¸² NaN (à¸–à¹‰à¸²à¸¡à¸µ)
    df.dropna(inplace=True)

    return df
# Sidebar for page selection
page = st.sidebar.radio("Select Page:", [ "ðŸ“Š Summarize ML", "ðŸ“ˆ Demo Stock Forecasting", "âš›ï¸ Summarize NL","ðŸ¤– Demo Neural Network"])

# ----------------------------- Page 1: Data -----------------------------
if page == "ðŸ“Š Summarize ML":
    st.title("ðŸ“Š Dataset US Stock From NASDAQ")
    st.markdown("ðŸ”— [Download Dataset](https://www.nasdaq.com/market-activity/stocks/msft/historical?page=1&rows_per_page=10&timeline=y1)")
    
    # à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œà¸«à¸£à¸·à¸­à¹ƒà¸Šà¹‰à¹„à¸Ÿà¸¥à¹Œà¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡
    uploaded_file = st.file_uploader("Upload your CSV file (Optional)", type=["csv"])
    if uploaded_file is not None:
        df = load_data(uploaded_file)
    else:
        file_path = "NVDA.csv"
        df = load_data(file_path)
    
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ðŸ“Š Summarize Data", "ðŸ“ˆ Data Cleaning Demo", "ðŸŒ  SVM", "ðŸŒ² RF", "Ref."])
    
    with tab1:
        st.write("### ðŸ”¹ Features in Dataset")
        
        feature_descriptions = {
            "Date": "ðŸ“… à¸§à¸±à¸™à¸—à¸µà¹ˆà¸—à¸µà¹ˆà¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (MM/DD/YYYY)",
            "Close/Last": "ðŸ’° à¸£à¸²à¸„à¸²à¸›à¸´à¸”à¸‚à¸­à¸‡à¸«à¸¸à¹‰à¸™à¹ƒà¸™à¸§à¸±à¸™à¸™à¸±à¹‰à¸™",
            "Volume": "ðŸ“Š à¸›à¸£à¸´à¸¡à¸²à¸“à¸à¸²à¸£à¸‹à¸·à¹‰à¸­à¸‚à¸²à¸¢à¸«à¸¸à¹‰à¸™à¹ƒà¸™à¸§à¸±à¸™à¸™à¸±à¹‰à¸™",
            "Open": "ðŸš€ à¸£à¸²à¸„à¸²à¹€à¸›à¸´à¸”à¸‚à¸­à¸‡à¸«à¸¸à¹‰à¸™à¹ƒà¸™à¸§à¸±à¸™à¸™à¸±à¹‰à¸™",
            "High": "ðŸ“ˆ à¸£à¸²à¸„à¸²à¸ªà¸¹à¸‡à¸ªà¸¸à¸”à¸‚à¸­à¸‡à¸«à¸¸à¹‰à¸™à¹ƒà¸™à¸§à¸±à¸™à¸™à¸±à¹‰à¸™",
            "Low": "ðŸ“‰ à¸£à¸²à¸„à¸²à¸•à¹ˆà¸³à¸ªà¸¸à¸”à¸‚à¸­à¸‡à¸«à¸¸à¹‰à¸™à¹ƒà¸™à¸§à¸±à¸™à¸™à¸±à¹‰à¸™"
        }
        
        st.table(df.head())
        
        for col in df.columns:
            if col in feature_descriptions:
                st.write(f"**{col}** - {feature_descriptions[col]}")

        st.write("### ðŸ“Š Summary Statistics")
        st.write(df.describe())

        st.write("### ðŸ›  Data Cleaning Process")
        st.write("1. à¸¥à¸šà¹à¸–à¸§à¸—à¸µà¹ˆà¸¡à¸µà¸„à¹ˆà¸² Missing Values")
        st.code("df.dropna(inplace=True)")
        st.write("2. à¹à¸›à¸¥à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸«à¹‰à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸£à¸¹à¸›à¹à¸šà¸šà¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡")
        st.code("df[col] = pd.to_numeric(df[col], errors='coerce')")
        st.write("3. à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸‹à¹‰à¸³à¸‹à¹‰à¸­à¸™")
        st.code("df.drop_duplicates(inplace=True)")
        st.write("4. à¸¥à¸šà¸„à¹ˆà¸² Outliers à¸”à¹‰à¸§à¸¢ IQR Method")
        st.code("Q1, Q3 = df['Volume'].quantile([0.25, 0.75])")
        st.code("IQR = Q3 - Q1")
        st.code("lower_bound = Q1 - 1.5 * IQR")
        st.code("upper_bound = Q3 + 1.5 * IQR")
        st.code("df = df[(df['Volume'] >= lower_bound) & (df['Volume'] <= upper_bound)]")
        st.write("5. à¹à¸ªà¸”à¸‡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥")
        

        # à¸à¸³à¸«à¸™à¸”à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸¥à¸‚
        numeric_cols = ["Close/Last", "Volume", "Open", "High", "Low"]
        
        # à¸¥à¸šà¸ªà¸±à¸à¸¥à¸±à¸à¸©à¸“à¹Œà¸žà¸´à¹€à¸¨à¸© à¹€à¸Šà¹ˆà¸™ "$" à¹à¸¥à¸°à¸Šà¹ˆà¸­à¸‡à¸§à¹ˆà¸²à¸‡ à¹à¸¥à¹‰à¸§à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ float à¸­à¸¢à¹ˆà¸²à¸‡à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢
        for col in numeric_cols:
            df[col] = df[col].replace({'\$': '', ',': '', ' ': ''}, regex=True)
            df[col] = pd.to_numeric(df[col], errors='coerce')  # à¹ƒà¸Šà¹‰ pd.to_numeric() à¹€à¸žà¸·à¹ˆà¸­à¸«à¸¥à¸µà¸à¹€à¸¥à¸µà¹ˆà¸¢à¸‡à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”


        # à¸¥à¸šà¹à¸–à¸§à¸—à¸µà¹ˆà¸¡à¸µà¸„à¹ˆà¸² NaN
        df_cleaned = df.dropna()



        # à¸„à¸³à¸™à¸§à¸“à¹€à¸›à¸­à¸£à¹Œà¹€à¸‹à¹‡à¸™à¸•à¹Œà¸à¸²à¸£à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡à¸‚à¸­à¸‡à¸£à¸²à¸„à¸²
        df_cleaned["Price Change (%)"] = ((df_cleaned["Close/Last"] - df_cleaned["Open"]) / df_cleaned["Open"]) * 100

        st.write("### ðŸ“‰ Daily Price Change Percentage (à¸«à¸¥à¸±à¸‡à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”)")
        st.write(df_cleaned[["Date", "Open", "Close/Last", "Price Change (%)"]])

        # à¹à¸ªà¸”à¸‡à¸à¸£à¸²à¸Ÿà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¥à¸±à¸‡à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”
        fig = px.line(df_cleaned, x="Date", y="Price Change (%)", title="ðŸ“‰ Daily Price Change (%) Over Time (à¸«à¸¥à¸±à¸‡à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”)")
        st.plotly_chart(fig)
        
        st.write("## Why using Boxplot manage outlier ?")
        st.write("ðŸ“Š à¹ƒà¸Šà¹‰ Boxplot à¹€à¸žà¸·à¹ˆà¸­à¹à¸ªà¸”à¸‡à¸à¸²à¸£à¸à¸£à¸°à¸ˆà¸²à¸¢à¸•à¸±à¸§à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ à¹à¸¥à¸°à¸Šà¹ˆà¸§à¸¢à¹ƒà¸™à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸„à¹ˆà¸² Outliers à¹ƒà¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥")
        st.image("asset/Boxplot.png", use_container_width=True)
        st.write("ðŸ” à¸ˆà¸²à¸à¸à¸£à¸²à¸Ÿ Boxplot à¸ˆà¸°à¹€à¸«à¹‡à¸™à¹„à¸”à¹‰à¸§à¹ˆà¸²à¸¡à¸µà¸„à¹ˆà¸² Outliers à¸—à¸µà¹ˆà¸¡à¸µ Upper = Q3 + 1.5 * IQR à¹à¸¥à¸° Lower = Q1 - 1.5 * IQR")
        
        



    with tab2:
        st.write("### ðŸ“Œ Raw Data (Before Cleaning)")
        st.write(df.head())
    
        # à¸•à¸±à¸§à¹€à¸¥à¸·à¸­à¸à¸à¸²à¸£à¸—à¸³ Data Cleaning
        st.subheader("ðŸ› ï¸ Choose Data Cleaning Method")
        remove_duplicates = st.checkbox("ðŸ—‘ï¸ Remove Duplicate Rows")
        remove_outliers = st.checkbox("ðŸ“‰ Remove Outliers (IQR Method)")
        
        # à¸—à¸³ Data Cleaning
        df_cleaned = df.copy()
        
        # à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‹à¹‰à¸³à¸‹à¹‰à¸­à¸™
        if remove_duplicates:
            df_cleaned = df_cleaned.drop_duplicates()
        
        
        # à¸¥à¸šà¸„à¹ˆà¸² Outliers
        if remove_outliers:
            Q1, Q3 = df_cleaned["Volume"].quantile([0.25, 0.75])
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            df_cleaned = df_cleaned[(df_cleaned["Volume"] >= lower_bound) & (df_cleaned["Volume"] <= upper_bound)]
        
        # à¹à¸ªà¸”à¸‡à¸œà¸¥à¸«à¸¥à¸±à¸‡à¸—à¸³ Data Cleaning
        st.write("### âœ… Cleaned Data (After Cleaning)")
        st.write(df_cleaned.head())
        
        # à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸ˆà¸³à¸™à¸§à¸™à¹à¸–à¸§à¸à¹ˆà¸­à¸™à¹à¸¥à¸°à¸«à¸¥à¸±à¸‡à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”
        st.write(f"ðŸ“Š Data Before Cleaning: {df.shape[0]} rows")
        st.write(f"ðŸ“Š Data After Cleaning: {df_cleaned.shape[0]} rows")
        
        # à¹à¸ªà¸”à¸‡à¸à¸£à¸²à¸Ÿà¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸à¸²à¸£à¸à¸£à¸°à¸ˆà¸²à¸¢à¸•à¸±à¸§à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
        st.subheader("ðŸ”Ž Data Distribution Before vs After Cleaning")
        fig_before = px.box(df, y=["Close/Last", "Open", "High", "Low", "Volume"], title="Before Cleaning")
        fig_after = px.box(df_cleaned, y=["Close/Last", "Open", "High", "Low", "Volume"], title="After Cleaning")
        st.plotly_chart(fig_before)
        st.plotly_chart(fig_after)
        
        st.write("âœ… **Data Cleaning Completed!**")
    
    with tab3:
        st.write("### SVM Algorithm")
        st.write("à¸„à¸·à¸­ à¹€à¸›à¹‡à¸™à¸«à¸™à¸¶à¹ˆà¸‡à¹ƒà¸™à¸­à¸±à¸¥à¸à¸­à¸£à¸´à¸˜à¸¶à¸¡à¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸‚à¸­à¸‡à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡ (Machine Learning) à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¸ªà¸³à¸«à¸£à¸±à¸š Classification à¹à¸¥à¸° Regression à¹‚à¸”à¸¢à¹€à¸‰à¸žà¸²à¸°à¸­à¸¢à¹ˆà¸²à¸‡à¸¢à¸´à¹ˆà¸‡à¹ƒà¸™à¸‡à¸²à¸™à¸—à¸µà¹ˆà¸¡à¸µà¸‚à¸™à¸²à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸¡à¹ˆà¹ƒà¸«à¸à¹ˆà¸¡à¸²à¸à¹à¸¥à¸°à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³à¸ªà¸¹à¸‡")
        st.write("à¸à¸³à¸«à¸™à¸” Open , High , Low à¹€à¸›à¹‡à¸™ Feture X à¹à¸¥à¸°à¸œà¸¡à¹„à¸”à¹‰à¸à¸³à¸«à¸™à¸” Close/Last à¹€à¸›à¹‡à¸™ label à¹à¸—à¸™à¸¥à¸‡à¹ƒà¸™ y à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸à¸™à¸±à¹‰à¸™à¸à¹‡à¸—à¸³à¸à¸²à¸£ split à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸šà¹ˆà¸‡à¹€à¸›à¹‡à¸™ train 80% à¸à¸±à¸š test 20% à¹‚à¸”à¸¢à¹ƒà¸«à¹‰ random state = 42")
        st.image("asset/SVM1.png", use_container_width=True)
        
        st.write("**âš¡ Kernel Trick à¸„à¸·à¸­à¸­à¸°à¹„à¸£?**")
        st.write("Kernel Trick à¹€à¸›à¹‡à¸™à¹€à¸—à¸„à¸™à¸´à¸„à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¹ƒà¸™à¸à¸²à¸£à¹à¸›à¸¥à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹à¸šà¹ˆà¸‡à¹„à¸”à¹‰à¹ƒà¸™à¸¡à¸´à¸•à¸´à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™à¹ƒà¸«à¹‰à¹„à¸›à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ à¸¡à¸´à¸•à¸´à¸—à¸µà¹ˆà¸ªà¸¹à¸‡à¸‚à¸¶à¹‰à¸™ (Higher Dimensional Space) à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸ªà¸²à¸¡à¸²à¸£à¸–à¹à¸šà¹ˆà¸‡à¸”à¹‰à¸§à¸¢à¹€à¸ªà¹‰à¸™à¸•à¸£à¸‡à¹„à¸”à¹‰")
        #à¹à¸šà¹ˆà¸‡à¹€à¸›à¹‡à¸™ Bullet Points
        st.write("ðŸ”¹ **Linear Kernel** âž à¹ƒà¸Šà¹‰à¸ªà¸³à¸«à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹à¸šà¹ˆà¸‡à¹„à¸”à¹‰à¸”à¹‰à¸§à¸¢à¹€à¸ªà¹‰à¸™à¸•à¸£à¸‡")
        st.write("ðŸ”¹ **Polynomial Kernel** âž à¹ƒà¸Šà¹‰à¸ªà¸³à¸«à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹€à¸ªà¹‰à¸™à¹à¸šà¹ˆà¸‡à¹à¸šà¸š Polynomial (à¸§à¸‡à¸à¸¥à¸¡)")
        st.write("ðŸ”¹ **RBF Kernel** âž à¹ƒà¸Šà¹‰à¸ªà¸³à¸«à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™à¹à¸¥à¸°à¹„à¸¡à¹ˆà¹€à¸›à¹‡à¸™à¹€à¸Šà¸´à¸‡à¹€à¸ªà¹‰à¸™ âœ…")
        st.write(" **ðŸ“Œ à¸ªà¸£à¸¸à¸›**")
        st.write("SVM à¹€à¸›à¹‡à¸™à¸­à¸±à¸¥à¸à¸­à¸£à¸´à¸˜à¸¶à¸¡à¸—à¸µà¹ˆà¸—à¸£à¸‡à¸žà¸¥à¸±à¸‡ à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹„à¸”à¹‰à¸—à¸±à¹‰à¸‡ Classification à¹à¸¥à¸° Regression à¹à¸•à¹ˆà¹‚à¸”à¸¢à¸—à¸±à¹ˆà¸§à¹„à¸›à¸™à¸´à¸¢à¸¡à¹ƒà¸Šà¹‰à¹ƒà¸™ Classification à¸¡à¸²à¸à¸à¸§à¹ˆà¸² à¹ƒà¸Šà¹‰ Hyperplane à¹à¸¥à¸° Margin à¹ƒà¸™à¸à¸²à¸£à¹à¸¢à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸­à¸­à¸à¸ˆà¸²à¸à¸à¸±à¸™ à¹à¸¥à¸°à¸ªà¸²à¸¡à¸²à¸£à¸–à¹ƒà¸Šà¹‰ Kernel Trick à¹€à¸žà¸·à¹ˆà¸­à¸—à¸³à¹ƒà¸«à¹‰à¹à¸¢à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™à¹„à¸”à¹‰à¸”à¸µà¸¢à¸´à¹ˆà¸‡à¸‚à¸¶à¹‰à¸™ ðŸš€")
        st.image("asset/SVM2.png", use_container_width=True)
        st.write("à¸à¸²à¸£à¸«à¸¢à¸´à¸š model Support Vector Machine à¹€à¸¥à¸·à¸­à¸à¹ƒà¸Šà¹‰à¹à¸šà¸š Regression à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸²à¸“à¸„à¹ˆà¸²à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ à¸ˆà¸°à¹ƒà¸«à¹‰à¸„à¹ˆà¸²à¸„à¸¥à¸²à¸”à¹€à¸„à¸¥à¸·à¹ˆà¸­à¸™à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸Šà¹ˆà¸§à¸‡à¸—à¸µà¹ˆà¸à¸³à¸«à¸™à¸” à¹‚à¸”à¸¢à¸–à¹‰à¸² à¸ˆà¸¸à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸£à¸°à¸¢à¸° Îµ à¸–à¸·à¸­à¸§à¹ˆà¸²à¹„à¸¡à¹ˆà¸¡à¸µ error à¹à¸•à¹ˆà¸–à¹‰à¸² à¸ˆà¸¸à¸”à¸—à¸µà¹ˆà¸­à¸¢à¸¹à¹ˆ à¸™à¸­à¸à¸Šà¹ˆà¸§à¸‡ Îµ à¸ˆà¸°à¸–à¸¹à¸ penalized à¸”à¹‰à¸§à¸¢ loss function à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸à¸™à¸±à¹‰à¸™à¸à¹‡à¸›à¸£à¸°à¹€à¸¡à¸´à¸™ model à¸”à¹‰à¸§à¸¢ à¸„à¹ˆà¸² mean absolute error")
        
        y_test_svm, y_pred_svm, mae_svm = train_svm_model(df_cleaned)
        
        st.write(f"âœ… **SVM Mean Absolute Error:** {mae_svm:.8f}")
        st.write(f"âœ… **SVM Predictions**")
        st.write(pd.DataFrame({"True Price": y_test_svm, "Predicted Price": y_pred_svm}).head())
        
    with tab4:
        st.write("### ðŸŒ² Random Forest Algorithm")
        st.write("à¹€à¸›à¹‡à¸™à¸­à¸±à¸¥à¸à¸­à¸£à¸´à¸˜à¸¶à¸¡ Machine Learning à¹à¸šà¸š Ensemble Learning à¸—à¸µà¹ˆà¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸¡à¹€à¸”à¸¥à¸ˆà¸²à¸ à¸«à¸¥à¸²à¸¢à¹† Decision Trees à¹à¸¥à¸°à¹ƒà¸Šà¹‰à¸à¸²à¸£à¹‚à¸«à¸§à¸•à¸«à¸£à¸·à¸­à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸ˆà¸²à¸à¸•à¹‰à¸™à¹„à¸¡à¹‰à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¹„à¸”à¹‰à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸—à¸µà¹ˆà¹à¸¡à¹ˆà¸™à¸¢à¸³à¸‚à¸¶à¹‰à¸™ ")
        st.write("à¸à¸³à¸«à¸™à¸” Open , High , Low à¹€à¸›à¹‡à¸™ Feture X à¹à¸¥à¸°à¸œà¸¡à¹„à¸”à¹‰à¸à¸³à¸«à¸™à¸” Close/Last à¹€à¸›à¹‡à¸™ label à¹à¸—à¸™à¸¥à¸‡à¹ƒà¸™ y à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸à¸™à¸±à¹‰à¸™à¸à¹‡à¸—à¸³à¸à¸²à¸£ split à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸šà¹ˆà¸‡à¹€à¸›à¹‡à¸™ train 80% à¸à¸±à¸š test 20% à¹‚à¸”à¸¢à¹ƒà¸«à¹‰ random state = 42")
        st.image("asset/RF1.png", use_container_width=True)
        st.write("#### ðŸŽ¯ à¸«à¸¥à¸±à¸à¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™à¸‚à¸­à¸‡ Random Forest")
        st.write("ðŸ—ï¸ 1. à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡à¸«à¸¥à¸²à¸¢à¸•à¹‰à¸™à¹„à¸¡à¹‰ (Multiple Decision Trees)")
        st.write("ðŸ”¹ Random Forest à¸ˆà¸°à¸ªà¸£à¹‰à¸²à¸‡ à¸«à¸¥à¸²à¸¢à¹† Decision Trees à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹à¸•à¸à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™à¹€à¸¥à¹‡à¸à¸™à¹‰à¸­à¸¢")
        st.write("ðŸ”¹ à¹à¸•à¹ˆà¸¥à¸°à¸•à¹‰à¸™à¹„à¸¡à¹‰à¸ˆà¸°à¸–à¸¹à¸à¹€à¸—à¸£à¸™à¸”à¹‰à¸§à¸¢ à¸Šà¸¸à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸¸à¹ˆà¸¡à¸šà¸²à¸‡à¸ªà¹ˆà¸§à¸™ (Bootstrap Sampling)")
        st.write("ðŸ”¹ à¹à¸•à¹ˆà¸¥à¸°à¸•à¹‰à¸™à¹„à¸¡à¹‰à¸ˆà¸°à¹€à¸¥à¸·à¸­à¸ Feature à¹à¸šà¸šà¸ªà¸¸à¹ˆà¸¡ à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¸ˆà¸¸à¸”à¹à¸•à¸à¹à¸‚à¸™à¸‡ (Split)")
        st.write("ðŸŽ² 2. à¸à¸²à¸£à¸£à¸§à¸¡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œ")
        st.write("ðŸ”¹ à¸ªà¸³à¸«à¸£à¸±à¸š **Classification** â†’ à¹ƒà¸Šà¹‰ Majority Vote (à¹€à¸¥à¸·à¸­à¸à¸œà¸¥à¸—à¸µà¹ˆà¸¡à¸µà¹€à¸ªà¸µà¸¢à¸‡à¹‚à¸«à¸§à¸•à¸¡à¸²à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”)")
        st.write("ðŸ”¹ à¸ªà¸³à¸«à¸£à¸±à¸š **Regression** â†’ à¹ƒà¸Šà¹‰ à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢ à¸‚à¸­à¸‡à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢à¸ˆà¸²à¸à¸—à¸¸à¸à¸•à¹‰à¸™à¹„à¸¡à¹‰ âœ…")
        st.image("asset/RF2.png", use_container_width=True)
        st.code("X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)")
        st.write("#### ðŸ” à¸­à¸˜à¸´à¸šà¸²à¸¢à¹‚à¸„à¹‰à¸”")
        st.write("1. à¹ƒà¸Šà¹‰ RandomForestRegressor(n_estimators=100) â†’ à¸ªà¸£à¹‰à¸²à¸‡ 100 à¸•à¹‰à¸™à¹„à¸¡à¹‰")
        st.write("2. à¹à¸šà¹ˆà¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸›à¹‡à¸™ Train/Test à¸”à¹‰à¸§à¸¢ train_test_split()")
        st.write("3. à¹€à¸—à¸£à¸™à¹‚à¸¡à¹€à¸”à¸¥ rf_model.fit(X_train, y_train)")
        st.write("4. à¸—à¸³à¸™à¸²à¸¢ y_pred_rf = rf_model.predict(X_test)")
        st.write("5. à¸§à¸±à¸”à¸„à¹ˆà¸²à¸„à¸§à¸²à¸¡à¸œà¸´à¸”à¸žà¸¥à¸²à¸” Mean Absolute Error (MAE)")
        st.write("à¸à¸²à¸£à¸£à¸§à¸¡à¸à¸±à¸™à¸‚à¸­à¸‡à¸«à¸¥à¸²à¸¢à¹† Decision Trees à¹‚à¸”à¸¢ à¸ªà¸£à¹‰à¸²à¸‡à¸«à¸¥à¸²à¸¢à¹† Decision Trees à¹à¸•à¹ˆà¸¥à¸°à¸•à¹‰à¸™à¹„à¸¡à¹‰à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸ªà¸¸à¹ˆà¸¡à¸¡à¸² à¸—à¸³à¹ƒà¸«à¹‰à¸¡à¸µà¸„à¸§à¸²à¸¡à¹à¸•à¸à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™ à¸ˆà¸²à¸à¸™à¸±à¹‰à¸™ à¹à¸•à¹ˆà¸¥à¸°à¸•à¹‰à¸™à¹„à¸¡à¹‰à¹€à¸¥à¸·à¸­à¸à¹ƒà¸Šà¹‰ à¸šà¸²à¸‡à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œà¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¸—à¸³à¹ƒà¸«à¹‰à¸•à¹‰à¸™à¹„à¸¡à¹‰à¹à¸•à¹ˆà¸¥à¸°à¸•à¹‰à¸™à¹„à¸¡à¹ˆà¹€à¸«à¸¡à¸·à¸­à¸™à¸à¸±à¸™ à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢ à¸£à¸§à¸¡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸‚à¸­à¸‡à¸—à¸¸à¸à¸•à¹‰à¸™à¹„à¸¡à¹‰ à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ à¹ƒà¸Šà¹‰à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸‚à¸­à¸‡à¸—à¸¸à¸à¸•à¹‰à¸™à¹„à¸¡à¹‰à¹€à¸›à¹‡à¸™à¸„à¹ˆà¸²à¸—à¸³à¸™à¸²à¸¢à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢ à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸à¸™à¸±à¹‰à¸™à¸œà¸¡à¸à¹‡à¸›à¸£à¸°à¹€à¸¡à¸´à¸™ model à¸”à¹‰à¸§à¸¢ à¸„à¹ˆà¸² mean absolute error")
        y_test_rf, y_pred_rf, mae_rf = train_rf_model(df_cleaned)
        
        st.write(f"âœ… **Random Forest Mean Absolute Error:** {mae_rf:.8f}")
        st.write(f"âœ… **Random Forest Predictions**")
        st.write(pd.DataFrame({"True Price": y_test_rf, "Predicted Price": y_pred_rf}).head())
    
    with tab5:
        st.link_button("ðŸ”— Random Forest Algorithm", "https://www.geeksforgeeks.org/random-forest-algorithm-in-machine-learning/")
        st.link_button("ðŸ”— Support Vector Machine Algorithm", "https://www.geeksforgeeks.org/support-vector-machine-algorithm/")
        st.link_button("ðŸ”— Data Cleaning", "https://1stcraft.com/what-is-data-cleansing/")
        st.link_button("ðŸ”— Stock NASDAQ Data-set", "https://www.nasdaq.com/market-activity/stocks/msft/historical?page=1&rows_per_page=10&timeline=y1")
        

# ----------------------------- Page 2: Stock Forecasting -----------------------------
elif page == "ðŸ“ˆ Demo Stock Forecasting":
    st.title("ðŸ“ˆ NVIDIA Stock Price Forecasting")

    # Choose to upload a file or use the demo file
    uploaded_file = st.file_uploader("Upload your CSV file (Optional)", type=["csv"])
    if uploaded_file is not None:
        df = load_data(uploaded_file)
    else:
        # Default demo file
        file_path = "NVDA.csv"
        df = load_data(file_path)

    # Remove outliers
    Q1, Q3 = df["Volume"].quantile([0.25, 0.75])
    IQR = Q3 - Q1
    lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR
    df_cleaned = df[(df["Volume"] >= lower_bound) & (df["Volume"] <= upper_bound)]

    # à¸—à¸³à¸™à¸²à¸¢à¸œà¸¥à¸”à¹‰à¸§à¸¢à¹‚à¸¡à¹€à¸”à¸¥ SVM à¹à¸¥à¸° RF
    y_test_svm, y_pred_svm, mae_svm = train_svm_model(df_cleaned)
    y_test_rf, y_pred_rf, mae_rf = train_rf_model(df_cleaned)

    # à¹à¸ªà¸”à¸‡à¸„à¹ˆà¸² MAE
    st.write(f"âœ… **SVM Mean Absolute Error:** {mae_svm:.8f}")
    st.write(f"âœ… **Random Forest Mean Absolute Error:** {mae_rf:.8f}")

    # à¸„à¸³à¸™à¸§à¸“à¸„à¹ˆà¸²à¸ªà¸¹à¸‡à¸ªà¸¸à¸” (Max), à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢ (Avg), à¹à¸¥à¸°à¸„à¹ˆà¸²à¸•à¹ˆà¸³à¸ªà¸¸à¸” (Min)
    svm_max, svm_avg, svm_min = np.max(y_pred_svm), np.mean(y_pred_svm), np.min(y_pred_svm)
    rf_max, rf_avg, rf_min = np.max(y_pred_rf), np.mean(y_pred_rf), np.min(y_pred_rf)

    # à¹à¸ªà¸”à¸‡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸ˆà¸²à¸à¹‚à¸¡à¹€à¸”à¸¥ SVM à¹à¸¥à¸° RF à¸žà¸£à¹‰à¸­à¸¡à¸à¸£à¸²à¸Ÿ
    st.subheader("ðŸ“‰ Model Prediction Comparison")
    tab1, tab2 = st.tabs(["ðŸ”´ SVM Predictions", "ðŸŸ¢ Random Forest Predictions"])
    
    st.line_chart(df_cleaned["Close/Last"])
    
    with tab1:
        st.markdown("### ðŸ”´ SVM Model")
        st.write(f"ðŸ“Œ **Max Price:** ${svm_max:.2f}")
        st.write(f"ðŸ“Œ **Avg Price:** ${svm_avg:.2f}")
        st.write(f"ðŸ“Œ **Min Price:** ${svm_min:.2f}")

        fig_svm = px.scatter(x=y_test_svm, y=y_pred_svm, labels={"x": "True Price", "y": "Predicted Price"},
                                title="SVM: Predicted vs True Price", color_discrete_sequence=["red"])
        st.plotly_chart(fig_svm)

    with tab2:
        st.markdown("### ðŸŸ¢ Random Forest Model")
        st.write(f"ðŸ“Œ **Max Price:** ${rf_max:.2f}")
        st.write(f"ðŸ“Œ **Avg Price:** ${rf_avg:.2f}")
        st.write(f"ðŸ“Œ **Min Price:** ${rf_min:.2f}")

        fig_rf = px.scatter(x=y_test_rf, y=y_pred_rf, labels={"x": "True Price", "y": "Predicted Price"},
                            title="Random Forest: Predicted vs True Price", color_discrete_sequence=["green"])
        st.plotly_chart(fig_rf)
        
# ----------------------------- Page 3: Summarize NL -----------------------------

elif page == "âš›ï¸ Summarize NL":
    st.title("âš›ï¸ Natural Language Processing")
    
    # à¹à¸ªà¸”à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸”à¸´à¸šà¸à¹ˆà¸­à¸™à¸à¸²à¸£ encode
    if st.checkbox("ðŸ” Show Raw Data"):
        st.subheader("ðŸ“Š Raw Data")
        st.write(raw_data.head())  # à¹à¸ªà¸”à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸”à¸´à¸šà¸—à¸µà¹ˆà¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸—à¸³à¸à¸²à¸£ encode
    
    tab1, tab2, tab3= st.tabs(["ðŸ“Š Summarize Data", "ðŸŒ² Neural Network", "Ref."])
    
    with tab1:
        st.write("### ðŸ”¹ Features in Dataset")
        
        feature_descriptions = {
            "Accident ID": "ðŸ”¢ à¸£à¸«à¸±à¸ªà¸­à¸¸à¸šà¸±à¸•à¸´à¹€à¸«à¸•à¸¸",
            "Date": "ðŸ“… à¸§à¸±à¸™à¸—à¸µà¹ˆà¹€à¸à¸´à¸”à¸­à¸¸à¸šà¸±à¸•à¸´à¹€à¸«à¸•à¸¸",
            "Time": "ðŸ•’ à¹€à¸§à¸¥à¸²à¹€à¸à¸´à¸”à¸­à¸¸à¸šà¸±à¸•à¸´à¹€à¸«à¸•à¸¸",
            "Location": "ðŸ“ à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆà¹€à¸à¸´à¸”à¸­à¸¸à¸šà¸±à¸•à¸´à¹€à¸«à¸•à¸¸",
            "Weather": "ðŸŒ¦ï¸ à¸ªà¸ à¸²à¸žà¸­à¸²à¸à¸²à¸¨",
            "Cause": "ðŸš— à¸ªà¸²à¹€à¸«à¸•à¸¸à¸‚à¸­à¸‡à¸­à¸¸à¸šà¸±à¸•à¸´à¹€à¸«à¸•à¸¸",
            "Casualties": "ðŸ¤• à¸ˆà¸³à¸™à¸§à¸™à¸œà¸¹à¹‰à¸šà¸²à¸”à¹€à¸ˆà¹‡à¸š",
            "Road Condition": "ðŸ›£ï¸ à¸ªà¸ à¸²à¸žà¸–à¸™à¸™",
        }
        st.table(raw_data.head())
        
        for col in raw_data.columns:
            if col in feature_descriptions:
                st.write(f"**{col}** - {feature_descriptions[col]}")
                
        st.write("### ðŸ“Š Summary Statistics")
        st.write(raw_data.describe())
        
        st.write("### ðŸ›  Data Cleaning Process")
        st.write("1. à¸¥à¸šà¹à¸–à¸§à¸—à¸µà¹ˆà¸¡à¸µà¸„à¹ˆà¸² Missing Values")
        st.code("df.dropna(inplace=True)")
        st.write("2. à¹à¸›à¸¥à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸«à¹‰à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸£à¸¹à¸›à¹à¸šà¸šà¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡")
        st.code("df[col] = pd.to_numeric(df[col], errors='coerce')")
        st.write("3. à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸‹à¹‰à¸³à¸‹à¹‰à¸­à¸™")
        st.code("df.drop_duplicates(inplace=True)")
        st.write("4. à¸¥à¸šà¸„à¹ˆà¸² Outliers à¸”à¹‰à¸§à¸¢ IQR Method")
        st.code("Q1, Q3 = df['Volume'].quantile([0.25, 0.75])")
        st.code("IQR = Q3 - Q1")
        st.code("lower_bound = Q1 - 1.5 * IQR")
        st.code("upper_bound = Q3 + 1.5 * IQR")
        st.code("df = df[(df['Volume'] >= lower_bound) & (df['Volume'] <= upper_bound)]")
        st.write("5. à¹à¸ªà¸”à¸‡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥")
        
        st.write("à¸¥à¸šà¹à¸–à¸§à¸—à¸µà¹ˆà¸¡à¸µà¸„à¹ˆà¸² Missing Values")
        st.code("df_cleaned = df.dropna()")
        
    with tab2:
        st.write("### ðŸŒ² Neural Network Algorithm")
        st.write("à¸à¸²à¸£à¸žà¸±à¸’à¸™à¸²à¸£à¸°à¸šà¸šà¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢à¸ªà¸²à¹€à¸«à¸•à¸¸à¹à¸¥à¸°à¸ˆà¸³à¸™à¸§à¸™à¸œà¸¹à¹‰à¸šà¸²à¸”à¹€à¸ˆà¹‡à¸šà¸ˆà¸²à¸à¸­à¸¸à¸šà¸±à¸•à¸´à¹€à¸«à¸•à¸¸à¸—à¸²à¸‡à¸–à¸™à¸™ à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ Neural Network à¹à¸šà¸š Multi-Output à¹€à¸›à¹‡à¸™à¸à¸²à¸£à¹ƒà¸Šà¹‰à¹€à¸—à¸„à¸™à¸´à¸„ Machine Learning à¹€à¸žà¸·à¹ˆà¸­à¸—à¸³à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢ 2 à¸›à¸£à¸°à¹€à¸ à¸—à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸™à¹€à¸§à¸¥à¸²à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™ à¹„à¸”à¹‰à¹à¸à¹ˆ à¸ªà¸²à¹€à¸«à¸•à¸¸à¸‚à¸­à¸‡à¸­à¸¸à¸šà¸±à¸•à¸´à¹€à¸«à¸•à¸¸ (Classification) à¹à¸¥à¸° à¸ˆà¸³à¸™à¸§à¸™à¸œà¸¹à¹‰à¸šà¸²à¸”à¹€à¸ˆà¹‡à¸š (Regression)")
        
        st.write("## à¹ƒà¸Šà¹‰ One-Hot Encoding à¸ªà¸³à¸«à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆ")
        st.write("""
        à¸à¸²à¸£ **One-Hot Encoding** à¹€à¸›à¹‡à¸™à¸à¸²à¸£à¹à¸›à¸¥à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆà¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸¥à¸‚à¹‚à¸”à¸¢à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹ƒà¸«à¸¡à¹ˆà¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸•à¹ˆà¸¥à¸°à¸›à¸£à¸°à¹€à¸ à¸—à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ à¹‚à¸”à¸¢à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸™à¸±à¹‰à¸™à¸ˆà¸°à¸¡à¸µà¸„à¹ˆà¸²à¹€à¸›à¹‡à¸™ `1` à¸«à¸²à¸à¹à¸–à¸§à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸™à¸±à¹‰à¸™à¸¡à¸µà¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆà¸™à¸±à¹‰à¸™ à¹à¸¥à¸° `0` à¸«à¸²à¸à¹„à¸¡à¹ˆà¸¡à¸µ
        à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¹€à¸Šà¹ˆà¸™ à¹ƒà¸™à¸à¸£à¸“à¸µà¸—à¸µà¹ˆà¹€à¸£à¸²à¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸ªà¸ à¸²à¸žà¸­à¸²à¸à¸²à¸¨ à¹€à¸Šà¹ˆà¸™ `Sunny`, `Rainy`, à¹à¸¥à¸° `Cloudy` à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸ˆà¸²à¸ One-Hot Encoding à¸ˆà¸°à¸¡à¸µ 3 à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹ƒà¸«à¸¡à¹ˆ:
        - `Weather_Condition_Sunny`
        - `Weather_Condition_Rainy`
        - `Weather_Condition_Cloudy`
        
        à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸ˆà¸°à¸¡à¸µà¸¥à¸±à¸à¸©à¸“à¸°à¸”à¸±à¸‡à¸™à¸µà¹‰:

        | Weather Condition | Weather_Condition_Sunny | Weather_Condition_Rainy | Weather_Condition_Cloudy |
        |-------------------|-------------------------|-------------------------|--------------------------|
        | Sunny             | 1                       | 0                       | 0                        |
        | Rainy             | 0                       | 1                       | 0                        |
        | Cloudy            | 0                       | 0                       | 1                        |
        | Sunny             | 1                       | 0                       | 0                        |

        à¸à¸²à¸£à¹ƒà¸Šà¹‰ **One-Hot Encoding** à¹€à¸«à¸¡à¸²à¸°à¸ªà¸³à¸«à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µà¸¥à¸³à¸”à¸±à¸šà¸—à¸µà¹ˆà¸Šà¸±à¸”à¹€à¸ˆà¸™ à¹€à¸Šà¹ˆà¸™ `à¸ªà¸µ`, `à¸ªà¸ à¸²à¸žà¸­à¸²à¸à¸²à¸¨` à¸«à¸£à¸·à¸­à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™à¸›à¸£à¸°à¹€à¸ à¸—à¸•à¹ˆà¸²à¸‡ à¹† à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¸ªà¸±à¸¡à¸žà¸±à¸™à¸˜à¹Œà¸—à¸²à¸‡à¸¥à¸³à¸”à¸±à¸š
        """)
        
        st.write("## à¸—à¸¤à¸©à¸Žà¸µà¸‚à¸­à¸‡à¸­à¸±à¸¥à¸à¸­à¸£à¸´à¸˜à¸¶à¸¡ (Algorithm Theory)")
        st.write("à¹ƒà¸™à¸à¸²à¸£à¸žà¸±à¸’à¸™à¸²à¹‚à¸¡à¹€à¸”à¸¥à¸™à¸µà¹‰à¹ƒà¸Šà¹‰ Neural Network à¹à¸šà¸š Multi-Output à¸‹à¸¶à¹ˆà¸‡à¸ªà¸²à¸¡à¸²à¸£à¸–à¸—à¸³à¸™à¸²à¸¢à¸—à¸±à¹‰à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸Šà¸´à¸‡à¸žà¸²à¸“à¸´à¸Šà¸¢à¹Œ (Classification) à¹à¸¥à¸°à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸Šà¸´à¸‡à¸„à¸“à¸´à¸•à¸¨à¸²à¸ªà¸•à¸£à¹Œ (Regression) à¸žà¸£à¹‰à¸­à¸¡à¸à¸±à¸™")
        st.write("ðŸ”¹ Classification: à¸—à¸³à¸™à¸²à¸¢ à¸ªà¸²à¹€à¸«à¸•à¸¸à¸‚à¸­à¸‡à¸­à¸¸à¸šà¸±à¸•à¸´à¹€à¸«à¸•à¸¸ à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰à¸à¸²à¸£à¸ªà¸¸à¹ˆà¸¡à¸•à¸±à¸§à¹€à¸¥à¸·à¸­à¸à¸«à¸¥à¸²à¸¢à¸›à¸£à¸°à¹€à¸ à¸—à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆ (Softmax Activation).")
        st.write("ðŸ”¹ Regression: à¸—à¸³à¸™à¸²à¸¢ à¸ˆà¸³à¸™à¸§à¸™à¸œà¸¹à¹‰à¸šà¸²à¸”à¹€à¸ˆà¹‡à¸š à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰à¸„à¹ˆà¸²à¸•à¹ˆà¸­à¹€à¸™à¸·à¹ˆà¸­à¸‡ (ReLU Activation).")
        st.write("à¹‚à¸¡à¹€à¸”à¸¥ Neural Network à¸™à¸µà¹‰à¸ˆà¸°à¸¡à¸µà¸Šà¸±à¹‰à¸™à¸‹à¹ˆà¸­à¸™à¸«à¸¥à¸²à¸¢à¸Šà¸±à¹‰à¸™ (Hidden Layers) à¸‹à¸¶à¹ˆà¸‡à¸—à¸³à¹ƒà¸«à¹‰à¸ªà¸²à¸¡à¸²à¸£à¸–à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸„à¸§à¸²à¸¡à¸ªà¸±à¸¡à¸žà¸±à¸™à¸˜à¹Œà¸—à¸µà¹ˆà¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸­à¸´à¸™à¸žà¸¸à¸• (Features) à¹à¸¥à¸°à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œ (Outputs)")

        st.write("## à¸à¸²à¸£à¸žà¸±à¸’à¸™à¸²à¹‚à¸¡à¹€à¸”à¸¥ (Model Development)")
        st.write("""
        à¹ƒà¸™à¸à¸²à¸£à¸žà¸±à¸’à¸™à¸²à¹‚à¸¡à¹€à¸”à¸¥à¸™à¸µà¹‰à¹€à¸£à¸²à¹ƒà¸Šà¹‰ **Neural Network à¹à¸šà¸š Multi-Output** à¹€à¸žà¸·à¹ˆà¸­à¸—à¸³à¸™à¸²à¸¢à¸—à¸±à¹‰à¸‡à¸ªà¸²à¹€à¸«à¸•à¸¸à¸‚à¸­à¸‡à¸­à¸¸à¸šà¸±à¸•à¸´à¹€à¸«à¸•à¸¸ (Classification) à¹à¸¥à¸°à¸ˆà¸³à¸™à¸§à¸™à¸œà¸¹à¹‰à¸šà¸²à¸”à¹€à¸ˆà¹‡à¸š (Regression) à¸žà¸£à¹‰à¸­à¸¡à¸à¸±à¸™à¹ƒà¸™à¹‚à¸¡à¹€à¸”à¸¥à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™
        à¹‚à¸¡à¹€à¸”à¸¥à¸™à¸µà¹‰à¸›à¸£à¸°à¸à¸­à¸šà¸”à¹‰à¸§à¸¢à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸«à¸¥à¸±à¸ à¹† à¸”à¸±à¸‡à¸™à¸µà¹‰:
        
        1. **à¸à¸²à¸£à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥**:
            - à¹ƒà¸Šà¹‰ **One-Hot Encoding** à¹€à¸žà¸·à¹ˆà¸­à¹à¸›à¸¥à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆ à¹€à¸Šà¹ˆà¸™ `Weather Condition` à¹à¸¥à¸° `Road Condition` à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸¥à¸‚
            - à¹ƒà¸Šà¹‰ **MinMaxScaler** à¹€à¸žà¸·à¹ˆà¸­à¸—à¸³à¸à¸²à¸£à¸›à¸£à¸±à¸šà¸ªà¹€à¸à¸¥à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸±à¸§à¹€à¸¥à¸‚ à¹€à¸Šà¹ˆà¸™ `Vehicles Involved`
        
        2. **à¸à¸²à¸£à¸­à¸­à¸à¹à¸šà¸šà¹‚à¸¡à¹€à¸”à¸¥**:
            - à¹‚à¸¡à¹€à¸”à¸¥à¸›à¸£à¸°à¸à¸­à¸šà¸”à¹‰à¸§à¸¢à¸Šà¸±à¹‰à¸™à¸‹à¹ˆà¸­à¸™à¸«à¸¥à¸²à¸¢à¸Šà¸±à¹‰à¸™ (Hidden Layers) à¹‚à¸”à¸¢à¹€à¸£à¸´à¹ˆà¸¡à¸ˆà¸²à¸à¸Šà¸±à¹‰à¸™ **Dense** à¸—à¸µà¹ˆà¸¡à¸µà¸ˆà¸³à¸™à¸§à¸™ 256 à¸«à¸™à¹ˆà¸§à¸¢ à¹ƒà¸Šà¹‰à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™ **ReLU** à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¹‚à¸¡à¹€à¸”à¸¥à¸ªà¸²à¸¡à¸²à¸£à¸–à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸„à¸§à¸²à¸¡à¸ªà¸±à¸¡à¸žà¸±à¸™à¸˜à¹Œà¸—à¸µà¹ˆà¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
            - à¹ƒà¸Šà¹‰ **BatchNormalization** à¹€à¸žà¸·à¹ˆà¸­à¸›à¸£à¸±à¸šà¹ƒà¸«à¹‰à¸„à¹ˆà¸²à¸•à¹ˆà¸²à¸‡ à¹† à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸Šà¹ˆà¸§à¸‡à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡
            - à¹ƒà¸Šà¹‰ **Dropout** à¹€à¸žà¸·à¹ˆà¸­à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸à¸²à¸£à¹€à¸à¸´à¸” Overfitting
            - à¹‚à¸¡à¹€à¸”à¸¥à¸¡à¸µ **2 Output Layers**: 
                - **Cause Output**: à¸ªà¸³à¸«à¸£à¸±à¸šà¸—à¸³à¸™à¸²à¸¢à¸ªà¸²à¹€à¸«à¸•à¸¸à¸‚à¸­à¸‡à¸­à¸¸à¸šà¸±à¸•à¸´à¹€à¸«à¸•à¸¸ (à¹ƒà¸Šà¹‰ **Softmax Activation**)
                - **Casualties Output**: à¸ªà¸³à¸«à¸£à¸±à¸šà¸—à¸³à¸™à¸²à¸¢à¸ˆà¸³à¸™à¸§à¸™à¸œà¸¹à¹‰à¸šà¸²à¸”à¹€à¸ˆà¹‡à¸š (à¹ƒà¸Šà¹‰ **ReLU Activation**)
        
        3. **à¸à¸²à¸£à¸à¸¶à¸à¸ªà¸­à¸™à¹‚à¸¡à¹€à¸”à¸¥**:
            - à¹ƒà¸Šà¹‰ **Adam Optimizer** à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸„à¸³à¸™à¸§à¸“à¸„à¹ˆà¸² Gradient Descent
            - à¹€à¸¥à¸·à¸­à¸ **sparse_categorical_crossentropy** à¹€à¸›à¹‡à¸™ Loss Function à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢à¸ªà¸²à¹€à¸«à¸•à¸¸ (Classification)
            - à¹€à¸¥à¸·à¸­à¸ **Mean Squared Error (MSE)** à¹€à¸›à¹‡à¸™ Loss Function à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢à¸ˆà¸³à¸™à¸§à¸™à¸œà¸¹à¹‰à¸šà¸²à¸”à¹€à¸ˆà¹‡à¸š (Regression)
            - à¹ƒà¸Šà¹‰ **Early Stopping** à¹€à¸žà¸·à¹ˆà¸­à¸«à¸¢à¸¸à¸”à¸à¸²à¸£à¸à¸¶à¸à¸«à¸²à¸à¹‚à¸¡à¹€à¸”à¸¥à¹„à¸¡à¹ˆà¸žà¸±à¸’à¸™à¸²à¹ƒà¸™à¸Šà¹ˆà¸§à¸‡à¸«à¸¥à¸²à¸¢ à¹† epoch
        
        4. **à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥**:
            - à¹ƒà¸Šà¹‰ **Accuracy** à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¸‚à¸­à¸‡à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢à¸ªà¸²à¹€à¸«à¸•à¸¸à¸‚à¸­à¸‡à¸­à¸¸à¸šà¸±à¸•à¸´à¹€à¸«à¸•à¸¸
            - à¹ƒà¸Šà¹‰ **Mean Absolute Error (MAE)** à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¸‚à¸­à¸‡à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢à¸ˆà¸³à¸™à¸§à¸™à¸œà¸¹à¹‰à¸šà¸²à¸”à¹€à¸ˆà¹‡à¸š
        """)
        st.write("## Code Implementation")
        st.code("""
                # à¸à¸¶à¸à¹‚à¸¡à¹€à¸”à¸¥
                history = model.fit(X_train, {"cause_output": y_cause_train, "casualties_output": y_casualties_train},
                    validation_data=(X_test, {"cause_output": y_cause_test, "casualties_output": y_casualties_test}),
                    epochs=200, batch_size=64, callbacks=[early_stopping])
                    """)
        st.code(""" 
                # à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¹‚à¸¡à¹€à¸”à¸¥
                loss, cause_loss, casualties_loss, cause_acc, casualties_mae = model.evaluate(X_test, {"cause_output": y_cause_test, "casualties_output": y_casualties_test})
                """)
        st.write("## à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸šà¸à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸«à¸¡à¹ˆ (Model Prediction)")
        st.code(""" 
                # à¸—à¸”à¸ªà¸­à¸šà¸à¸²à¸£à¸žà¸¢à¸²à¸à¸£à¸“à¹Œà¸à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸«à¸¡à¹ˆ
                sample_data = np.array(X_test.iloc[:5])
                predicted_causes, predicted_casualties = model.predict(sample_data)
                """)
    with tab3:
        st.write("### ðŸ“š References")
        st.link_button("ðŸ”— One-Hot Encoding","https://www.geeksforgeeks.org/ml-one-hot-encoding/")
        st.link_button("ðŸ”— Neural Network", "https://www.geeksforgeeks.org/neural-networks-a-beginners-guide/")
        st.link_button("ðŸ”— Neural Network multi-output", "https://medium.com/@kevinnjagi83/building-deep-learning-models-with-multi-output-architectures-61d1c3c81d40")
        st.link_button("ðŸ”— Softmax Activation", "https://medium.com/super-ai-engineer/softmax-function-%E0%B8%84%E0%B8%B7%E0%B8%AD%E0%B8%AD%E0%B8%B0%E0%B9%84%E0%B8%A3-eae1f1bbef63")
        st.link_button("ðŸ”— ReLU Activation", "https://www.geeksforgeeks.org/relu-activation-function-in-deep-learning/")
        st.link_button("ðŸ”— Adam Optimizer", "https://www.geeksforgeeks.org/adam-optimizer/")
        st.link_button("ðŸ”— Sheet in classroom", "https://classroom.google.com/u/1/w/NzMyNDg3MzcyNzEy/t/all")
        st.link_button("ðŸ”— Data set", "https://www.kaggle.com/datasets/adilshamim8/global-traffic-accidents-dataset")
# ----------------------------- Page 4: Neural Network -----------------------------
elif page == "ðŸ¤– Demo Neural Network":
    st.title("ðŸ¤– Accident Prediction")
    
    # à¹à¸ªà¸”à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸”à¸´à¸šà¸à¹ˆà¸­à¸™à¸à¸²à¸£ encode
    if st.checkbox("ðŸ” Show Raw Data"):
        st.subheader("ðŸ“Š Raw Data")
        st.write(raw_data.head())  # à¹à¸ªà¸”à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸”à¸´à¸šà¸—à¸µà¹ˆà¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸—à¸³à¸à¸²à¸£ encode
    st.write(f"ðŸ”¹ **MAE: {history.history['casualties_output_mae'][-1]}**")
    st.write(f"ðŸ”¹ **Cause Prediction Accuracy: {history.history['cause_output_accuracy'][-1]}**")
    
    tab1, tab2, tab3, tab4 = st.tabs(["ðŸ“Š Training History", "ðŸ“ˆ Cause Prediction Accuracy", "ðŸ”¥ Cause Accident", "ðŸ§  Sample Predictions"])
    with tab1:
        st.subheader("ðŸ“Š Training History")
        fig = go.Figure()
        fig.add_trace(go.Scatter(y=history.history['loss'], mode='lines+markers', name='Total Loss'))
        fig.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines+markers', name='Validation Loss'))
        fig.update_layout(xaxis_title='Epochs', yaxis_title='Loss', template='plotly_dark')
        st.plotly_chart(fig)

    with tab2:
        st.subheader("ðŸ“ˆ Cause Prediction Accuracy")
        fig = go.Figure()
        fig.add_trace(go.Scatter(y=history.history['cause_output_accuracy'], mode='lines+markers', name='Train Accuracy'))
        fig.add_trace(go.Scatter(y=history.history['val_cause_output_accuracy'], mode='lines+markers', name='Validation Accuracy'))
        fig.update_layout(xaxis_title='Epochs', yaxis_title='Accuracy', template='plotly_dark')
        st.plotly_chart(fig)
    
    with tab3:
        st.subheader("ðŸ“Š Cause Comparison: 2023 vs 2024")
        causes = ["Reckless Driving", "Drunk Driving", "Weather Conditions", "Speeding", "Mechanical Failure", "Distracted Driving"]
        data_2023 = [100, 150, 80, 200, 50, 120]  # Example data for 2023
        data_2024 = [120, 160, 90, 220, 55, 130]  # Example data for 2024
        df_comparison = pd.DataFrame({
            "Cause": causes,
            "2023": data_2023,
            "2024": data_2024
        })
        fig = px.bar(df_comparison, x="Cause", y=["2023", "2024"], barmode="group", title="Cause Comparison: 2023 vs 2024")
        fig.update_layout(xaxis_title="Cause", yaxis_title="Total Count", template="plotly_dark")
        st.plotly_chart(fig)
    
    with tab4:
        st.subheader("ðŸ§  Sample Predictions")
        sample_data = np.array(X_test.iloc[:5])
        predicted_causes, predicted_casualties = model.predict(sample_data)
        predicted_causes = np.argmax(predicted_causes, axis=1)
        predicted_causes = cause_encoder.inverse_transform(predicted_causes)
        predicted_casualties = predicted_casualties.flatten()
        
        predictions_df = pd.DataFrame({
            "Predicted Cause": predicted_causes,
            "Predicted Casualties": predicted_casualties
        })
        st.dataframe(predictions_df.style.format({"Predicted Casualties": "{:.0f}"}).set_properties(**{'text-align': 'center'}))
